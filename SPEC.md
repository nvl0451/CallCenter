# CallCenter LLM + RAG + Vision ‚Äî One‚ÄëShot Codex CLI

–ì–æ—Ç–æ–≤—ã–π one‚Äëshot –ø–∞–∫–µ—Ç: —Å–∫–æ—Ä–º–∏—Ç–µ —ç—Ç–æ—Ç –¥–æ–∫—É–º–µ–Ω—Ç **codex CLI** –∏ –ø–æ–ª—É—á–∏—Ç–µ —Ä–∞–±–æ—á–µ–µ —Ä–µ—à–µ–Ω–∏–µ.

> **–ß—Ç–æ –≤–Ω—É—Ç—Ä–∏**
> - FastAPI‚Äë—Å–µ—Ä–≤–∏—Å —Å LLM‚Äë–∞–≥–µ–Ω—Ç–æ–º (OpenAI, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é `gpt-5-mini`, –ø–æ–¥–¥–µ—Ä–∂–∫–∞ Chat/Responses)
> - –ü—Ä–æ–º–ø—Ç‚Äë—Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Ç–∏–ø–æ–≤ –∑–∞–ø—Ä–æ—Å–æ–≤ (–ø–æ–¥–¥–µ—Ä–∂–∫–∞/–ø—Ä–æ–¥–∞–∂–∏/–∂–∞–ª–æ–±—ã)
> - –•—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–∏–∞–ª–æ–≥–∞ (SQLite)
> - RAG: ChromaDB + sentence‚Äëtransformers (—ç–º–±–µ–¥–¥–∏–Ω–≥–∏) + QA endpoint
> - Vision: zero‚Äëshot –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (CLIP —á–µ—Ä–µ–∑ `open-clip-torch`)
> - –ú–µ—Ç—Ä–∏–∫–∏/–ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ, –±–∞–∑–æ–≤–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–æ —Å—Ç–æ–∏–º–æ—Å—Ç–∏/–≤—Ä–µ–º–µ–Ω–∏

---

## üìÅ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞
```
callcenter-ai/
‚îú‚îÄ app/
‚îÇ  ‚îú‚îÄ __init__.py
‚îÇ  ‚îú‚îÄ main.py
‚îÇ  ‚îú‚îÄ config.py
‚îÇ  ‚îú‚îÄ llm.py
‚îÇ  ‚îú‚îÄ prompts.py
‚îÇ  ‚îú‚îÄ memory.py
‚îÇ  ‚îú‚îÄ rag.py
‚îÇ  ‚îú‚îÄ vision.py
‚îÇ  ‚îî‚îÄ schemas.py
‚îú‚îÄ data/
‚îÇ  ‚îú‚îÄ kb/
‚îÇ  ‚îÇ  ‚îú‚îÄ pricing.md
‚îÇ  ‚îÇ  ‚îú‚îÄ refund_policy.md
‚îÇ  ‚îÇ  ‚îî‚îÄ troubleshooting.md
‚îú‚îÄ chroma_db/        # –ª–æ–∫–∞–ª—å–Ω–∞—è –ø–∞–ø–∫–∞ –¥–ª—è Chroma (persist)
‚îú‚îÄ requirements.txt
‚îú‚îÄ .env.example
‚îú‚îÄ README.md
‚îî‚îÄ run.sh
```

---

## ‚öôÔ∏è requirements.txt
```txt
fastapi>=0.111
uvicorn[standard]>=0.30
httpx>=0.27
pydantic>=2.7
python-dotenv>=1.0
chromadb>=0.5.5
sentence-transformers>=3.0
numpy>=1.26
pandas>=2.2
scikit-learn>=1.4
# Torch stack (CPU)
torch>=2.3; platform_system!="Darwin" or platform_machine!="arm64"
torchvision>=0.18; platform_system!="Darwin" or platform_machine!="arm64"
# macOS arm64 users: —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ torch –≤—Ä—É—á–Ω—É—é –ø–æ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ PyTorch
Pillow>=10.3
open-clip-torch>=2.24.0
python-multipart>=0.0.9
```

---

## üîê .env.example
```env
# –ö–ª—é—á OpenAI
OPENAI_API_KEY=sk-...
# –ú–æ–¥–µ–ª—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é (gpt‚Äë5 —Ç—Ä–µ–±—É–µ—Ç Responses API)
OPENAI_MODEL=gpt-5-mini
# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å OpenAI Responses API (1) –∏–ª–∏ Chat Completions (0)
OPENAI_USE_RESPONSES=1

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ë–î
SQLITE_PATH=./callcenter.sqlite3

# ChromaDB
CHROMA_DIR=./chroma_db

# –¢—é–Ω–∏–Ω–≥–∏ LLM
LLM_TEMPERATURE=0.2
# –õ–∏–º–∏—Ç—ã –Ω–∞ —Ç–æ–∫–µ–Ω—ã (–æ—Ç–≤–µ—Ç / –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è)
REPLY_MAX_TOKENS=256
CLASSIFY_MAX_TOKENS=16
```

---

## üöÄ run.sh
```bash
#!/usr/bin/env bash
set -euo pipefail
export PYTHONUNBUFFERED=1
if [ -f .env ]; then export $(grep -v '^#' .env | xargs); fi
uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
```

---

## üß† app/config.py
```python
from __future__ import annotations
import os
from pydantic import BaseModel

class Settings(BaseModel):
    # OpenAI
    openai_api_key: str = os.getenv("OPENAI_API_KEY", "")
    openai_model: str = os.getenv("OPENAI_MODEL", "gpt-5-mini")
    openai_use_responses: bool = bool(int(os.getenv("OPENAI_USE_RESPONSES", "1")))

    sqlite_path: str = os.getenv("SQLITE_PATH", "./callcenter.sqlite3")
    chroma_dir: str = os.getenv("CHROMA_DIR", "./chroma_db")

    llm_temperature: float = float(os.getenv("LLM_TEMPERATURE", "0.2"))
    reply_max_tokens: int = int(os.getenv("REPLY_MAX_TOKENS", "256"))
    classify_max_tokens: int = int(os.getenv("CLASSIFY_MAX_TOKENS", "16"))

settings = Settings()
```

---

## üßæ app/schemas.py
```python
from __future__ import annotations
from pydantic import BaseModel, Field
from typing import List, Optional

class StartDialogRequest(BaseModel):
    customer_id: Optional[str] = None
    metadata: Optional[dict] = None

class StartDialogResponse(BaseModel):
    session_id: str

class MessageRequest(BaseModel):
    session_id: str
    message: str = Field(..., min_length=1)

class MessageResponse(BaseModel):
    type: str
    reply: str
    cost_estimate_usd: float
    latency_ms: int

class IngestRequest(BaseModel):
    documents: Optional[List[str]] = None  # —Å—ã—Ä—ã–µ —Ç–µ–∫—Å—Ç—ã

class AskRequest(BaseModel):
    question: str
    top_k: int = 4
    session_id: Optional[str] = None

class AskResponse(BaseModel):
    answer: str
    sources: List[dict]

class VisionResponse(BaseModel):
    category: str
    confidence: float
    logits: List[float]
    labels: List[str]
```

---

## üß© app/prompts.py
```python
SYSTEM_BASE = (
    "–¢—ã ‚Äî –≤–µ–∂–ª–∏–≤—ã–π –∏ —Ç–æ—á–Ω—ã–π LLM‚Äë–∞–≥–µ–Ω—Ç –∫–æ–ª–ª‚Äë—Ü–µ–Ω—Ç—Ä–∞. –ì–æ–≤–æ—Ä–∏ –∫—Ä–∞—Ç–∫–æ, –¥—Ä—É–∂–µ–ª—é–±–Ω–æ, –ø–æ –¥–µ–ª—É. "
    "–ï—Å–ª–∏ –Ω—É–∂–Ω–∞ —É—Ç–æ—á–Ω—è—é—â–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è ‚Äî –∑–∞–¥–∞–π 1-2 –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–∞."
)

CLASSIFY_PROMPT = (
    "–ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∑–∞–ø—Ä–æ—Å –≤ –æ–¥–Ω—É –∫–∞—Ç–µ–≥–æ—Ä–∏—é –∏–∑: "
    "[—Ç–µ—Ö–ø–æ–¥–¥–µ—Ä–∂–∫–∞, –ø—Ä–æ–¥–∞–∂–∏, –∂–∞–ª–æ–±–∞]. –í–µ—Ä–Ω–∏ —Ç–æ–ª—å–∫–æ –æ–¥–Ω–æ —Å–ª–æ–≤–æ –∏–∑ —Å–ø–∏—Å–∫–∞.\n\n–ó–∞–ø—Ä–æ—Å: {text}"
)

PROMPTS_BY_TYPE = {
    "—Ç–µ—Ö–ø–æ–¥–¥–µ—Ä–∂–∫–∞": (
        "–¢—ã —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç —Ç–µ—Ö–ø–æ–¥–¥–µ—Ä–∂–∫–∏. –ü–æ–ø—Ä–æ—Å–∏ —É –∫–ª–∏–µ–Ω—Ç–∞ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–µ—Ç–∞–ª–∏, "
        "–¥–∞–π 1-3 —à–∞–≥–∞ —Ä–µ—à–µ–Ω–∏—è, –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –ø—Ä–µ–¥–ª–æ–∂–∏ —ç—Å–∫–∞–ª–∞—Ü–∏—é."
    ),
    "–ø—Ä–æ–¥–∞–∂–∏": (
        "–¢—ã –º–µ–Ω–µ–¥–∂–µ—Ä –ø–æ –ø—Ä–æ–¥–∞–∂–∞–º. –£—Ç–æ—á–Ω–∏ –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç–∏, –ø—Ä–µ–¥–ª–æ–∂–∏ –ø–æ–¥—Ö–æ–¥—è—â–∏–π —Ç–∞—Ä–∏—Ñ/–ø–∞–∫–µ—Ç, "
        "—Å—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π —á—ë—Ç–∫–∏–π next step (–¥–µ–º–æ, —Å—á—ë—Ç, –ø—Ä–æ–±–Ω—ã–π –ø–µ—Ä–∏–æ–¥)."
    ),
    "–∂–∞–ª–æ–±–∞": (
        "–¢—ã —Å–æ—Ç—Ä—É–¥–Ω–∏–∫ –ø–æ —Ä–∞–±–æ—Ç–µ —Å –∂–∞–ª–æ–±–∞–º–∏. –ü—Ä–∏–∑–Ω–∞–π –ø—Ä–æ–±–ª–µ–º—É, –∏–∑–≤–∏–Ω–∏—Å—å, –æ–ø–∏—à–∏, —á—Ç–æ —Å–¥–µ–ª–∞–µ—à—å, "
        "–ø—Ä–µ–¥–ª–æ–∂–∏ –∫–æ–º–ø–µ–Ω—Å–∞—Ü–∏—é –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –∏ —Å—Ä–æ–∫–∏ –æ—Ç–≤–µ—Ç–∞."
    ),
}
```

---

## üßÆ app/memory.py (SQLite –∫–æ–Ω—Ç–µ–∫—Å—Ç)
```python
from __future__ import annotations
import sqlite3, pathlib, time
from typing import List, Tuple
from .config import settings

DB_PATH = pathlib.Path(settings.sqlite_path)

def init_db():
    conn = sqlite3.connect(DB_PATH)
    cur = conn.cursor()
    cur.execute(
        """
        CREATE TABLE IF NOT EXISTS messages (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            session_id TEXT NOT NULL,
            role TEXT NOT NULL,
            content TEXT NOT NULL,
            ts REAL NOT NULL
        )
        """
    )
    conn.commit(); conn.close()

init_db()

def add_message(session_id: str, role: str, content: str):
    conn = sqlite3.connect(DB_PATH)
    conn.execute(
        "INSERT INTO messages(session_id, role, content, ts) VALUES(?,?,?,?)",
        (session_id, role, content, time.time()),
    )
    conn.commit(); conn.close()

def get_session_messages(session_id: str, limit: int = 20) -> List[Tuple[str,str]]:
    conn = sqlite3.connect(DB_PATH)
    cur = conn.cursor()
    cur.execute(
        "SELECT role, content FROM messages WHERE session_id=? ORDER BY id DESC LIMIT ?",
        (session_id, limit),
    )
    rows = cur.fetchall()
    conn.close()
    return list(reversed(rows))
```

---

## üåê app/llm.py (OpenAI –∫–ª–∏–µ–Ω—Ç + –¥–∏–∞–ª–æ–≥)
```python
from __future__ import annotations
from typing import List, Dict
from openai import OpenAI
from .config import settings

class LLMClient:
    def __init__(self):
        self.model = settings.openai_model
        self.client = OpenAI(api_key=settings.openai_api_key) if settings.openai_api_key else None

    async def chat(self, messages: List[Dict[str, str]], max_tokens: int | None = None, temperature: float | None = None):
        if not self.client:
            return "[offline] –ù–µ—Ç OPENAI_API_KEY", 0, 0.0
        # gpt‚Äë5 ‚Üí Responses API —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º reasoning, –∏–Ω–∞—á–µ ‚Äî Chat Completions
        if settings.openai_use_responses and self.model.startswith("gpt-5"):
            text = "\n\n".join(f"{m['role'].upper()}: {m['content']}" for m in messages)
            resp = self.client.responses.create(
                model=self.model,
                input=text,
                max_output_tokens=max_tokens or settings.reply_max_tokens,
                reasoning={"effort": "minimal"},
                text={"verbosity": "low"},
            )
            return (resp.output_text or "").strip(), 0, 0.0
        else:
            resp = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                max_tokens=max_tokens or settings.reply_max_tokens,
                temperature=settings.llm_temperature if temperature is None else temperature,
            )
            return (resp.choices[0].message.content or "").strip(), 0, 0.0

llm_client = LLMClient()
```

---

## üîé app/rag.py (ChromaDB + sentence‚Äëtransformers)
```python
from __future__ import annotations
import os, uuid, pathlib
from typing import List, Dict
import chromadb
from chromadb.config import Settings as ChromaSettings
from sentence_transformers import SentenceTransformer
from .config import settings

CHROMA_DIR = pathlib.Path(settings.chroma_dir)
CHROMA_DIR.mkdir(parents=True, exist_ok=True)

client = chromadb.Client(ChromaSettings(is_persistent=True, persist_directory=str(CHROMA_DIR)))
collection = client.get_or_create_collection("company_kb")
embedder = SentenceTransformer("all-MiniLM-L6-v2")

def embed_texts(texts: List[str]) -> List[List[float]]:
    return embedder.encode(texts, normalize_embeddings=True).tolist()

def ingest_texts(texts: List[str], metadatas: List[Dict] | None = None):
    ids = [str(uuid.uuid4()) for _ in texts]
    embs = embed_texts(texts)
    collection.add(ids=ids, embeddings=embs, documents=texts, metadatas=metadatas)
    client.persist()
    return ids

def search(query: str, top_k: int = 4):
    q_emb = embed_texts([query])[0]
    res = collection.query(query_embeddings=[q_emb], n_results=top_k, include=["documents", "metadatas", "distances", "ids"])
    items = []
    for i in range(len(res["ids"][0])):
        items.append({
            "id": res["ids"][0][i],
            "document": res["documents"][0][i],
            "metadata": res.get("metadatas", [[{}]])[0][i] or {},
            "score": float(1.0 - (res.get("distances", [[1.0]])[0][i] or 1.0)),
        })
    return items
```

---

## üñºÔ∏è app/vision.py (Zero‚Äëshot CLIP –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è)
```python
from __future__ import annotations
from typing import List, Tuple
from PIL import Image
import torch
import open_clip

CATEGORIES = [
    "–æ—à–∏–±–∫–∞ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞",
    "–ø—Ä–æ–±–ª–µ–º–∞ —Å –æ–ø–ª–∞—Ç–æ–π",
    "—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π —Å–±–æ–π",
    "–≤–æ–ø—Ä–æ—Å –ø–æ –ø—Ä–æ–¥—É–∫—Ç—É",
    "–¥—Ä—É–≥–æ–µ",
]

_model = None
_preprocess = None
_tokenizer = None

def _load():
    global _model, _preprocess, _tokenizer
    if _model is None:
        model, _, preprocess = open_clip.create_model_and_transforms(
            model_name="ViT-B-32", pretrained="openai"
        )
        tokenizer = open_clip.get_tokenizer("ViT-B-32")
        model.eval()
        _model, _preprocess, _tokenizer = model, preprocess, tokenizer

@torch.inference_mode()
def classify_image(img: Image.Image) -> Tuple[str, float, List[float], List[str]]:
    _load()
    image = _preprocess(img).unsqueeze(0)
    texts = _tokenizer([f"—Ñ–æ—Ç–æ: {c}" for c in CATEGORIES])
    with torch.no_grad():
        image_features = _model.encode_image(image)
        text_features = _model.encode_text(texts)
        image_features /= image_features.norm(dim=-1, keepdim=True)
        text_features /= text_features.norm(dim=-1, keepdim=True)
        logits = (100.0 * image_features @ text_features.T).softmax(dim=-1).squeeze(0)
    conf, idx = float(logits.max().item()), int(logits.argmax().item())
    return CATEGORIES[idx], conf, [float(x) for x in logits.tolist()], CATEGORIES
```

---

## üß≠ app/main.py (FastAPI endpoints)
```python
from __future__ import annotations
import uuid
from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from .schemas import *
from .config import settings
from .prompts import SYSTEM_BASE, CLASSIFY_PROMPT, PROMPTS_BY_TYPE
from .memory import add_message, get_session_messages
from .llm import llm_client
from .rag import ingest_texts, search
from .vision import classify_image
from PIL import Image

app = FastAPI(title="CallCenter LLM + RAG + Vision")
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], allow_credentials=True,
    allow_methods=["*"], allow_headers=["*"],
)

@app.get("/health")
def health():
    return {"ok": True}

# ---------- –î–∏–∞–ª–æ–≥ ----------
@app.post("/dialog/start", response_model=StartDialogResponse)
def start_dialog(req: StartDialogRequest):
    session_id = str(uuid.uuid4())
    add_message(session_id, "system", SYSTEM_BASE)
    if req.metadata:
        add_message(session_id, "system", f"meta:{req.metadata}")
    return StartDialogResponse(session_id=session_id)

async def _classify(text: str) -> str:
    # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —á–µ—Ä–µ–∑ LLM: –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Å—Ç—Ä–æ–≥–æ–µ –æ–¥–Ω–æ —Å–ª–æ–≤–æ
    messages = [
        {"role": "system", "content": SYSTEM_BASE},
        {"role": "user", "content": CLASSIFY_PROMPT.format(text=text)},
    ]
    out, _, _ = await llm_client.chat(messages, max_tokens=4, temperature=0.0)
    cat = out.strip().lower()
    cat = {"—Ç–µ—Ö–ø–æ–¥–¥–µ—Ä–∂–∫–∞":"—Ç–µ—Ö–ø–æ–¥–¥–µ—Ä–∂–∫–∞", "–ø–æ–¥–¥–µ—Ä–∂–∫–∞":"—Ç–µ—Ö–ø–æ–¥–¥–µ—Ä–∂–∫–∞", "support":"—Ç–µ—Ö–ø–æ–¥–¥–µ—Ä–∂–∫–∞",
           "sales":"–ø—Ä–æ–¥–∞–∂–∏", "–ø—Ä–æ–¥–∞–∂–∏":"–ø—Ä–æ–¥–∞–∂–∏",
           "–∂–∞–ª–æ–±–∞":"–∂–∞–ª–æ–±–∞", "complaint":"–∂–∞–ª–æ–±–∞"}.get(cat, "—Ç–µ—Ö–ø–æ–¥–¥–µ—Ä–∂–∫–∞")
    return cat

@app.post("/dialog/message", response_model=MessageResponse)
async def send_message(req: MessageRequest):
    session_id, user_text = req.session_id, req.message
    # –∫–æ–Ω—Ç–µ–∫—Å—Ç
    history = get_session_messages(session_id)
    if not history:
        raise HTTPException(400, "unknown session_id")
    add_message(session_id, "user", user_text)

    # –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
    category = await _classify(user_text)
    sys_prompt = PROMPTS_BY_TYPE.get(category, "")

    messages = [{"role": "system", "content": SYSTEM_BASE + "\n" + sys_prompt}]
    # –ü–æ–¥–º–µ—à–∏–≤–∞–µ–º –∫—Ä–∞—Ç–∫–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 8 —Å–æ–æ–±—â–µ–Ω–∏–π)
    for role, content in history[-8:]:
        if role in ("user", "assistant"):
            messages.append({"role": role, "content": content})
    messages.append({"role": "user", "content": user_text})

    reply, latency_ms, cost = await llm_client.chat(messages)
    add_message(session_id, "assistant", reply)

    return MessageResponse(type=category, reply=reply, cost_estimate_usd=cost, latency_ms=latency_ms)

# ---------- RAG ----------
@app.post("/rag/ingest")
def rag_ingest(req: IngestRequest):
    texts = req.documents or []
    # –ï—Å–ª–∏ –Ω–µ –ø–µ—Ä–µ–¥–∞–ª–∏ ‚Äî –∑–∞–ª—å—ë–º –¥–µ–º–æ —Ñ–∞–π–ª—ã
    if not texts:
        demo = []
        for p in ["data/kb/pricing.md", "data/kb/refund_policy.md", "data/kb/troubleshooting.md"]:
            try:
                with open(p, "r", encoding="utf-8") as f:
                    demo.append(f.read())
            except FileNotFoundError:
                pass
        texts = demo
    ids = ingest_texts(texts, metadatas=[{"source":"ingest"} for _ in texts])
    return {"count": len(ids), "ids": ids}

@app.post("/rag/ask", response_model=AskResponse)
async def rag_ask(req: AskRequest):
    docs = search(req.question, top_k=req.top_k)
    # –°—Ñ–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–¥—Å–∫–∞–∑–∫—É —Å —Ü–∏—Ç–∞—Ç–∞–º–∏
    context = "\n\n".join([f"[DOC {i+1}]\n" + d["document"] for i, d in enumerate(docs)])
    sys = SYSTEM_BASE + "\n–û—Ç–≤–µ—á–∞–π, –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ–ª—å–∫–æ —Ñ–∞–∫—Ç—ã –∏–∑ [DOC]. –ï—Å–ª–∏ —á–µ–≥–æ‚Äë—Ç–æ –Ω–µ—Ç –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö ‚Äî —á–µ—Å—Ç–Ω–æ —Å–∫–∞–∂–∏."
    messages = [
        {"role": "system", "content": sys},
        {"role": "user", "content": f"–í–æ–ø—Ä–æ—Å: {req.question}\n\n–ö–æ–Ω—Ç–µ–∫—Å—Ç:\n{context}"},
    ]
    answer, _, _ = await llm_client.chat(messages)
    return AskResponse(answer=answer, sources=[{"id": d["id"], "score": d["score"]} for d in docs])

# ---------- Vision ----------
@app.post("/vision/classify", response_model=VisionResponse)
async def classify(file: UploadFile = File(...)):
    if not file.content_type.startswith("image/"):
        raise HTTPException(400, "upload an image file")
    img = Image.open(file.file).convert("RGB")
    category, conf, logits, labels = classify_image(img)
    return VisionResponse(category=category, confidence=conf, logits=logits, labels=labels)
```

---

## üìù data/kb/pricing.md (–ø—Ä–∏–º–µ—Ä)
```md
–¢–∞—Ä–∏—Ñ—ã: –ë–∞–∑–æ–≤—ã–π (10$/–º–µ—Å), –ü–ª—é—Å (25$/–º–µ—Å), –ë–∏–∑–Ω–µ—Å (99$/–º–µ—Å). –°–∫–∏–¥–∫–∞ 20% –ø—Ä–∏ –æ–ø–ª–∞—Ç–µ –∑–∞ –≥–æ–¥. –í—Å–µ —Ç–∞—Ä–∏—Ñ—ã –≤–∫–ª—é—á–∞—é—Ç –ø–æ–¥–¥–µ—Ä–∂–∫—É –ø–æ email, –ë–∏–∑–Ω–µ—Å ‚Äî –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–π –æ—Ç–≤–µ—Ç –≤ —Ç–µ—á–µ–Ω–∏–µ 2 —á–∞—Å–æ–≤.
```

## üìù data/kb/refund_policy.md (–ø—Ä–∏–º–µ—Ä)
```md
–í–æ–∑–≤—Ä–∞—Ç —Å—Ä–µ–¥—Å—Ç–≤ –≤–æ–∑–º–æ–∂–µ–Ω –≤ —Ç–µ—á–µ–Ω–∏–µ 14 –¥–Ω–µ–π –ø–æ—Å–ª–µ –ø–æ–∫—É–ø–∫–∏ –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–±–ª–µ–º, –∫–æ—Ç–æ—Ä—ã–µ –º—ã –Ω–µ —Å–º–æ–≥–ª–∏ —Ä–µ—à–∏—Ç—å. –ó–∞–ø—Ä–æ—Å –æ—Ñ–æ—Ä–º–ª—è–µ—Ç—Å—è —á–µ—Ä–µ–∑ –ø–æ–¥–¥–µ—Ä–∂–∫—É, —Å—Ä–æ–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏ ‚Äî –¥–æ 5 —Ä–∞–±–æ—á–∏—Ö –¥–Ω–µ–π.
```

## üìù data/kb/troubleshooting.md (–ø—Ä–∏–º–µ—Ä)
```md
–ï—Å–ª–∏ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –Ω–µ –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç—Å—è: –ø–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ, –æ—á–∏—Å—Ç–∏—Ç–µ –∫–µ—à, –ø–µ—Ä–µ—É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ. –î–ª—è –æ—à–∏–±–æ–∫ –æ–ø–ª–∞—Ç—ã: –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–∞—Ä—Ç—É, –ª–∏–º–∏—Ç—ã, –ø–æ–≤—Ç–æ—Ä–∏—Ç–µ –ø–æ–ø—ã—Ç–∫—É —á–µ—Ä–µ–∑ 10 –º–∏–Ω—É—Ç –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –¥—Ä—É–≥–æ–π —Å–ø–æ—Å–æ–±.
```

---

## üìö README.md
```md
# CallCenter LLM + RAG + Vision

## –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç
1. Python 3.10+
2. `python -m venv .venv && source .venv/bin/activate`
3. `pip install -r requirements.txt`
4. –°–æ–∑–¥–∞–π—Ç–µ `.env` –∏–∑ `.env.example` –∏ –ø—Ä–æ–ø–∏—à–∏—Ç–µ `OPENAI_API_KEY` (–∏ `OPENAI_MODEL` –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏)
5. `bash run.sh`

## –ü—Ä–∏–º–µ—Ä—ã –∑–∞–ø—Ä–æ—Å–æ–≤
### –î–∏–∞–ª–æ–≥
```bash
curl -sX POST http://localhost:8000/dialog/start -H 'Content-Type: application/json' -d '{}'
# => {"session_id":"..."}

curl -sX POST http://localhost:8000/dialog/message -H 'Content-Type: application/json' \
  -d '{"session_id":"<ID>", "message":"—É –º–µ–Ω—è —Å–ø–∏—Å–∞–ª–∏ –¥–µ–Ω—å–≥–∏ –¥–≤–∞–∂–¥—ã"}'
```

### RAG
```bash
curl -sX POST http://localhost:8000/rag/ingest -H 'Content-Type: application/json' -d '{}'

curl -sX POST http://localhost:8000/rag/ask -H 'Content-Type: application/json' \
  -d '{"question":"–∫–∞–∫–∞—è –ø–æ–ª–∏—Ç–∏–∫–∞ –≤–æ–∑–≤—Ä–∞—Ç–∞?"}'
```

### Vision
```bash
curl -s -F 'file=@screenshot.png' http://localhost:8000/vision/classify
```

## –ó–∞–º–µ—Ç–∫–∏ –ø–æ —Å—Ç–æ–∏–º–æ—Å—Ç–∏/—Å–∫–æ—Ä–æ—Å—Ç–∏
- –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ = 0.2; REPLY_MAX_TOKENS –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 256.
- –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏–¥—ë—Ç –∫–æ—Ä–æ—Ç–∫–∏–º –≤—ã–∑–æ–≤–æ–º LLM —Å `CLASSIFY_MAX_TOKENS=16`, `temperature=0.0`.
- –î–ª—è gpt‚Äë5 –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è OpenAI Responses —Å reasoning `minimal`.
- –ú–æ–∂–Ω–æ –≤–∫–ª—é—á–∏—Ç—å –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ —Ç—Ä–∏–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ (—Å–º. `history[-8:]`).

## –ú–µ—Ç—Ä–∏–∫–∏/–ª–æ–≥–∏
- –í –æ—Ç–≤–µ—Ç–∞—Ö –¥–∏–∞–ª–æ–≥–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç—Å—è –ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –∏ latency.
- –î–ª—è –ø—Ä–æ–¥–∞–∫—à–µ–Ω–∞ –ø–æ–¥–∫–ª—é—á–∏—Ç–µ Prometheus/OTel.
```

---

## üß™ app/__init__.py
```python
# –ø—É—Å—Ç–æ
```

---

### –ü–æ–¥—Å–∫–∞–∑–∫–∏ –¥–ª—è Codex CLI
- –°–æ—Ö—Ä–∞–Ω–∏—Ç–µ —ç—Ç–æ—Ç –¥–æ–∫—É–º–µ–Ω—Ç –∫–∞–∫ `SPEC.md` –≤ –ø—É—Å—Ç–æ–π –ø–∞–ø–∫–µ.
- –ó–∞–ø—É—Å—Ç–∏—Ç–µ –≤–∞—à—É –∫–æ–º–∞–Ω–¥—É Codex/Codegen —Å —Ä–µ–∂–∏–º–æ–º —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è –∏–∑ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–∏.
- –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Å–æ–∑–¥–∞—Å—Ç –æ–ø–∏—Å–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã –∏ –∑–∞–ø–æ–ª–Ω–∏—Ç –∏—Ö —Å–æ–¥–µ—Ä–∂–∏–º—ã–º.

–£–¥–∞—á–∏ –∏ –º—è–≥–∫–∏—Ö SLA! üßÉ
